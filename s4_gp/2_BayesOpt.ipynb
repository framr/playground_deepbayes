{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian optimization\n",
    "\n",
    "* Mainly used for optimization of \"heavy\" functions (computationally complex, expensive to evaluate)\n",
    "* The objective function can be \"black box\"\n",
    "* Uses approximation of the objective function\n",
    "* Takes into account quality of the approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Optimization procedure:\n",
    "1. Build approximation $\\hat{f}(x)$ of function $f(x)$\n",
    "2. Choose new point as an argmax of the criterion\n",
    "$$\n",
    "x_{new} = \\arg\\max\\limits_x a(x)\n",
    "$$\n",
    "3. Evaluate $f(x)$ at new point\n",
    "4. Update model $\\hat{f}(x)$ and go to step 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Expected Improvement\n",
    "\n",
    "$$\n",
    "EI(x) = \\mathbb{E}_{p(\\hat{f})} \\left [\\max(0, y_{min} - \\hat{f}) \\right ]\n",
    "$$\n",
    "where $\\hat{y}, \\sigma$ - mean and variance of GP model at point $x$,\n",
    "$\\Phi(\\cdot)$ - cdf of standard normal distribution,\n",
    "$\\phi(\\cdot)$ - pdf of standard normal distribution.\n",
    "\n",
    "Usually logarithm of EI is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"EI_vs_logEI.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization of criterion\n",
    "\n",
    "Any optimization algorithm could be used.\n",
    "\n",
    "In this seminar we will use multi-start with L-BFGS optimization algorithm\n",
    "\n",
    "Multi-start procedure:\n",
    "1. Generate initial set of points $x_1, \\ldots, x_n$. Calculate criterion at each point to obtain $(a(x_1), \\ldots, a(x_n))$.\n",
    "2. Choose $k$ points with smallest values of criterion.\n",
    "3. Using each point as an initial point run the optimization algorithm (L-BFGS) and obtain $k$ optimization results.\n",
    "4. From all optimization results choose the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### L-BFGS \n",
    "\n",
    "It's a quasi-Newton method of optimization and it is based on second order Taylor expansion\n",
    "$$\n",
    "f(x_k + p) \\approx f(x_k) + \\nabla f^T(x_k) p + \\frac12 p^T \\mathbf{H}p\n",
    "$$\n",
    "$$\n",
    "p = -\\mathbf{H}^{-1}\\nabla f^T(x_k) \\approx -\\mathbf{B}_k^{-1} \\nabla f^T(x_k),\n",
    "$$\n",
    "where $\\mathbf{B}_k$ is an approximation of hessian $\\mathbf{H}$.\n",
    "\n",
    "Approximation $\\mathbf{B}_k$ is updated at every step by the following rule:\n",
    "$$\n",
    "\\mathbf{B}_{k + 1} = \\mathbf{B}_k - \\frac{\\mathbf{B}_k s_k s_k^T \\mathbf{B}_k}{s_k^T \\mathbf{B}_k s_k} + \\frac{y_k y_k^T}{y_k^T s_k},\n",
    "$$\n",
    "where $s_k = x_{k + 1} - x_k$, $y_k = \\nabla f(x_{k + 1}) - \\nabla f(x_k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "import GPy\n",
    "\n",
    "import bayes_opt\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return (6 * x - 2)**2 * np.sin(12 * x - 4)  \n",
    "\n",
    "def get_1d_data():\n",
    "    np.random.seed(239)\n",
    "    x_train = np.array([0.0, 0.58, 0.38, 0.95]).reshape(-1, 1)\n",
    "    y_train = f(x_train)\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x_train, y_train = get_1d_data()\n",
    "kernel = GPy.kern.RBF(1, variance=0.5, lengthscale=0.2)\n",
    "model = GPy.models.GPRegression(x_train, y_train, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x_grid = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "y_grid = f(x_grid)\n",
    "prediction, std = model.predict(x_grid)\n",
    "prediction = prediction.ravel()\n",
    "std = std.ravel()\n",
    "pyplot.figure(figsize=(8, 6))\n",
    "pyplot.plot(x_train, y_train, 'or', markersize=8, label='Training set')\n",
    "pyplot.plot(x_grid, prediction, '-k', linewidth=2, label='Approximation')\n",
    "pyplot.fill_between(x_grid.ravel(), prediction - 2 * std, prediction + 2 * std, alpha=0.3)\n",
    "pyplot.plot(x_grid, y_grid, '--b', label='True function')\n",
    "pyplot.ylim([-15, 20])\n",
    "pyplot.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Derive expression for EI: express it in terms of $\\Phi(\\cdot)$ and $\\phi(\\cdot)$ - cdf and pdf of $\\mathcal{N}(0, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Task\n",
    "Implement multi-start optimization procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def get_new_point(model, lb, ub, data=None, multistart=10, criterion='ei', k=1, random_state=None):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        model - GP model of the objective function\n",
    "        lb, ub - array-like, lower and upper bounds of x\n",
    "        data - tuple(x_train, y_train)\n",
    "        multistart - number of multistart runs\n",
    "        criterion - aqcuisition function, by default EI\n",
    "        k - parameter of the LowerConfidenceBound function\n",
    "        random_state - np.random.RandomState\n",
    "    Returns\n",
    "        tuple - argmin of the objective function and min value of the objective \n",
    "    \"\"\"\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState()\n",
    "\n",
    "    lb = np.array(lb).reshape(1, -1)\n",
    "    ub = np.array(ub).reshape(1, -1)\n",
    "    \n",
    "    # 1. Generate inital X points (number of points == multistart) in [lb, ub]\n",
    "    \n",
    "    ######## Your code here ########\n",
    "    x_random = \n",
    "\n",
    "    \n",
    "    ######## ########\n",
    "    def objective(x):\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "        mean_values, variance = model.predict(x)\n",
    "        std_values = np.sqrt(variance)\n",
    "        if criterion == 'ei':\n",
    "            return -log_expected_improvement(mean_values, std_values, data[1].min())\n",
    "        elif criterion == 'lcb':\n",
    "            return lcb(mean_values, std_values, params)\n",
    "        else:\n",
    "            raise NotImplementedError('Criterion is not implemented!')\n",
    "\n",
    "    criterion_value = objective(x_random)\n",
    "    \n",
    "    # 2. From each points from x_random run L-BFGS optimization algorithm, \n",
    "    #    choose the best result and return it\n",
    "    #    Use function minimize: minimize(objective, x_init, method='L-BFGS-B',\n",
    "    #                                    bounds=np.vstack((lb, ub)).T)\n",
    "    #    it returns object with fields 'fun' - optimum function value, 'x' - argmin.\n",
    "\n",
    "    best_result = None\n",
    "    best_value = np.inf\n",
    "\n",
    "    ######## Your code here ########\n",
    "    \n",
    "    return best_result.x, best_result.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Check your code \n",
    "lb = [0]\n",
    "ub = [1]\n",
    "kernel = GPy.kern.RBF(1, variance=0.5, lengthscale=0.1)\n",
    "model = GPy.models.GPRegression(x_train, y_train, kernel)\n",
    "x_new, f_new = get_new_point(model, lb, ub, data=(x_train, y_train), random_state=np.random.RandomState(42))\n",
    "\n",
    "assert(np.isclose(x_new, 0.29985639))\n",
    "assert(np.isclose(f_new, 0.86480674))\n",
    "print('Correct!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimization_step(x_train, y_train, kernel, objective, lb=None, ub=None, criterion='ei', k=1, plot=False):\n",
    "    model = GPy.models.GPRegression(x_train, y_train, kernel)\n",
    "    model.optimize_restarts(num_restarts=10, verbose=False)\n",
    "\n",
    "    x_new, criterion_value = get_new_point(model, data=(x_train, y_train), lb=lb, ub=ub, criterion=criterion, k=k)\n",
    "    if plot:\n",
    "        bayes_opt.plot1d(x_train, y_train, model, objective, x_new, criterion_value)\n",
    "        pyplot.show()\n",
    "\n",
    "    x_new = x_new.reshape(1, -1)\n",
    "    x_train = np.vstack([x_train, x_new])\n",
    "    y_train = np.vstack([y_train, np.asarray(objective(x_new)).reshape(1, -1)])\n",
    "    return x_train, y_train, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 1D example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x_train, y_train = get_1d_data()\n",
    "kernel = GPy.kern.RBF(1, variance=0.5, lengthscale=0.2)\n",
    "model = GPy.models.GPRegression(x_train, y_train, kernel)\n",
    "for i in range(6):\n",
    "    x_train, y_train, model = bayes_opt.optimization_step(x_train, y_train, kernel, f, lb=[0], ub=[1], criterion='ei', plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 2D demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "budget = 30\n",
    "n_init = 10\n",
    "\n",
    "kernel = GPy.kern.RBF(2, variance=1, lengthscale=0.5, ARD=False)\n",
    "\n",
    "save_path = '2d_demo.mp4'\n",
    "bayes_opt.demo_2d(n_init, budget, kernel, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "video = io.open(save_path, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hyperparmeters tuning\n",
    "\n",
    "* Almost all machine learning have hyperparameters\n",
    "* Quality of the model depends on the hyperparameters\n",
    "* Quality estimation for one set of hyperparameters can take long time\n",
    "* => Bayesian optimization can be used for hyperparameters tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bayesian optimization for hyperparameter tuning\n",
    "\n",
    "Objective function to optimize\n",
    "* Takes hyperparameters as input\n",
    "* Builds a model (maybe several times in case of cross-validation)\n",
    "* Calculates and returns model quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### House pricing dataset\n",
    "\n",
    "In this task you need to predict House Sale Price. There are 25 numerical input features like lot area, overall condition rating, house quality, number of kitchens and so on (there were a lot of categorical variables which we removed in this example for simplicity).\n",
    "\n",
    "We are going to tune XGBoost parameters using Bayesian Optimization to obtain more accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('house_pricing.csv')\n",
    "\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We implement `model_error_cv()` function that will be our objective function.  \n",
    "We are going to use RBF kernel in our Bayesian Optimization, the result of optimization will be continuous variables,\n",
    "so we need to preprocess parameters - cast integer parameters to int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def wrap_parameters(parameters, scaler=None):\n",
    "    if scaler:\n",
    "        parameters = scaler.transform(parameters)\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def unwrap_parameters(parameters, scaler=None):\n",
    "    if scaler:\n",
    "        parameters = scaler.inverse_transform(parameters)\n",
    "    p = [int(parameters[0]), parameters[1], int(parameters[2]),\n",
    "         max(0, min(parameters[3], 1))]\n",
    "    return p\n",
    "\n",
    "\n",
    "def model_error_cv(parameters, X, y, scaler=None):\n",
    "    errors = []\n",
    "    for p in parameters:\n",
    "        p = unwrap_parameters(p, scaler)\n",
    "        model = xgboost.XGBRegressor(max_depth=p[0],\n",
    "                                     learning_rate=p[1],\n",
    "                                     n_estimators=p[2],\n",
    "                                     subsample=p[3],\n",
    "                                     )\n",
    "\n",
    "        score = cross_val_score(model, X, y, cv=3).mean()\n",
    "        errors.append(score)\n",
    "    return np.array(errors).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We scale the parameters using StandardScaler() from sklearn - it is nice to have all the parameters with unit variance and mean zero\n",
    "when using RBF kernel as it is easier to tune lengthscale parameters, because these parameters depend on the range of input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# xgboost params: max_depth, learning_rate, n_estimators, subsample\n",
    "lower_bound = np.array([1, 0.001, 100, 0.2])\n",
    "upper_bound = np.array([6, 0.1, 1000, 1])\n",
    "\n",
    "np.random.seed(42)\n",
    "n_init_points = 10\n",
    "initial_parameters = np.random.rand(n_init_points, len(lower_bound)) * (upper_bound - lower_bound) + lower_bound\n",
    "initial_errors = -model_error_cv(initial_parameters, X, y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(initial_parameters)\n",
    "lower_bound = scaler.transform(lower_bound)\n",
    "upper_bound = scaler.transform(upper_bound)\n",
    "initial_parameters = wrap_parameters(initial_parameters, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It is also nice idea to explicitly constrain lengthscale parameter - it shouldn't be much larger than distance between points in the training set, it shouldn't be much smaller than the distance between points in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "kernel = GPy.kern.RBF(len(lower_bound), lengthscale=(upper_bound - lower_bound).min() / n_init_points, ARD=False)\n",
    "gp_model = GPy.models.GPRegression(initial_parameters, initial_errors, kernel=kernel)\n",
    "gp_model.rbf.lengthscale.constrain_bounded(0.001, 10)\n",
    "gp_model.optimize()\n",
    "print(gp_model)\n",
    "print(gp_model.rbf.lengthscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "budget = 40\n",
    "\n",
    "hyperparameters = initial_parameters\n",
    "errors = initial_errors\n",
    "error_history = [-initial_errors[:i].min() for i in range(1, n_init_points + 1)]\n",
    "objective = lambda x: -model_error_cv(x, X, y, scaler)\n",
    "for i in range(budget):\n",
    "    hyperparameters, errors, gp_model = bayes_opt.optimization_step(hyperparameters, errors, kernel, objective,\n",
    "                                                                    lb=lower_bound, ub=upper_bound)\n",
    "    error_history.append(-errors.min())\n",
    "    # Visualize\n",
    "    display.clear_output(wait=True)\n",
    "    pyplot.figure(figsize=(8, 6))\n",
    "        \n",
    "    pyplot.xlabel(\"#iteration\")\n",
    "    pyplot.ylabel(\"R2\")\n",
    "    pyplot.plot(error_history)\n",
    "    pyplot.show()\n",
    "    \n",
    "    print(\"New parameters: {}, new error:\\t{}\\nbest parameters: {}, best error:\\t{}\".format(\n",
    "        unwrap_parameters(hyperparameters[-1], scaler), -errors[-1, 0],\n",
    "        unwrap_parameters(hyperparameters[errors.argmin()], scaler), -errors.min()))\n",
    "    print(gp_model.rbf.lengthscale)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
